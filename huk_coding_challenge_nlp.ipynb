{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9622992,"sourceType":"datasetVersion","datasetId":5873607}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# uninstall wandb to fine-tune on Kaggle without token\n%pip uninstall --yes wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install and import all necessary libraries\n%pip install datasets\n%pip install evaluate\n\nimport datasets\nimport evaluate\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch","metadata":{"id":"MYhDz5Ie-fBc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if GPU is available\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Device Availble: {DEVICE}')","metadata":{"id":"yeSw1UFOCAui","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data as pandas dataframe without using first line as header\ntraining_data = pd.read_csv(\"/kaggle/input/huk-dataset/training.csv\", header=None)\nvalidation_data = pd.read_csv(\"/kaggle/input/huk-dataset/validation.csv\", header=None)","metadata":{"id":"amLunrst-v98","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rename column names\ntraining_data.rename(columns={0: 'ID', 1: 'product', 2: 'labels', 3: 'text'}, inplace=True)\nvalidation_data.rename(columns={0: 'ID', 1: 'product', 2: 'labels', 3: 'text'}, inplace=True)","metadata":{"id":"xLmAV7To-7Cy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep only data used for model training and rename labels to correct training data format\ntraining_data_final = training_data.loc[:, ['text', 'labels']].dropna()\ntraining_data_final['labels'] = training_data_final['labels'].map({'Negative': 0, 'Positive': 1, 'Neutral': 2, 'Irrelevant': 3})\n\nvalidation_data_final = validation_data.loc[:, ['text', 'labels']].dropna()\nvalidation_data_final['labels'] = validation_data_final['labels'].map({'Negative': 0, 'Positive': 1, 'Neutral': 2, 'Irrelevant': 3})","metadata":{"id":"DU8EarOz_dvi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize dataset\ntrain_dataset = datasets.Dataset.from_pandas(training_data_final)\ntest_dataset = datasets.Dataset.from_pandas(validation_data_final)\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized_train_data = train_dataset.map(tokenize_function, batched=True)\ntokenized_test_data = test_dataset.map(tokenize_function, batched=True)\n\naccuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    # Setup evaluation\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    acc = accuracy.compute(predictions=predictions, references=labels)\n    f1_score = f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n    return {\"accuracy\": acc, \"f1\": f1_score}\n\n# Load pretrained model and evaluate model after each epoch\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/output\",\n    eval_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_data,\n    eval_dataset=tokenized_test_data,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\ntrainer.evaluate()\n\nmodel.save_pretrained(\"/kaggle/working/HUK_model_v1\")\ntokenizer.save_pretrained(\"/kaggle/working/HUK_model_v1\")","metadata":{"id":"Tz8fwctL_rT6","trusted":true},"execution_count":null,"outputs":[]}]}